*GPU Utilization & Resource Allocation in Heterogeneous Clusters*

As big data processing systems and deep learning models have become increasingly significant and applied to many real-world applications, graphical processing unit (GPU) clusters with in-memory processing have become the primary processor choice for 
powerful high performance computing systems featuring in-memory capabilities for deep networks.

In order to process big data at the rates and volumes mentioned, not only have new techniques been developed such as Spark, Hadoop, and MapReduce, but advances in distributed cloud computing and clustering have led to a hyperbolic expansion of data centers 
around the globe.  Combined with parallel computing architectures of GPUs, cloud computing infrastructures have contributed to accelerated training and inference often expected of deep model networks. This robust parallelization for tensor processing and 
neural networks presents GPU clusters as a preferred processing solution for many supervised and unsupervised AI/ML tasks. A common framework for distributed clusters consists of varying models of GPUs and CPUs to form heterogeneous GPU clusters. 
